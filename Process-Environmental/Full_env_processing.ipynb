{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import ast\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HomeData class\n",
    "This is the parent for the following four classes and gathers the home information (home number, color, etc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomeData():\n",
    "    def __init__(self, path):\n",
    "        self.root_dir = path\n",
    "        self.write_dir = self.make_storage_directory(os.path.join(self.root_dir, 'Summaries'))\n",
    "        self.home = path.split('/')[-1].split('-')[-2]\n",
    "        #print(path.split('/'))\n",
    "        self.system = path.split('/')[-1].split('-')[-1]\n",
    "        self.average_length = 1\n",
    "    \n",
    "    def mylistdir(self, directory):\n",
    "        filelist = os.listdir(directory)\n",
    "        return [x for x in filelist if not (x.startswith('.') or 'Icon' in x)] \n",
    "\n",
    "    def make_storage_directory(self, target_dir):\n",
    "        if not os.path.exists(target_dir):\n",
    "            os.makedirs(target_dir)\n",
    "        return target_dir\n",
    "    \n",
    "    def date_segments(self, dates):\n",
    "        output = []\n",
    "        cur_list = [dates[0]]\n",
    "        for dt_pair in zip(dates[1:], dates):\n",
    "            if (dt_pair[0] - dt_pair[1]).days > 1:\n",
    "                output.append(cur_list)\n",
    "                cur_list = [dt_pair[0]]\n",
    "            else:\n",
    "                cur_list.append(dt_pair[0])\n",
    "        output.append(cur_list)\n",
    "        return output   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HomeOccupancy class\n",
    "This reads in the raw ground truth filese and creates the occupancy dataframes.\n",
    "Writes a summary df at the specified frequency.\n",
    "Stores in the ```Summary``` folder with the title structure ```H1-black-Occupancy_df.csv```\n",
    "\n",
    "eg:\n",
    "```\n",
    "path = '/Users/maggie/Desktop/HPD_mobile_data/HPD_mobile-H6/H6-black'\n",
    "o = HomeOccupancy(path)\n",
    "o.main()\n",
    "o.df\n",
    "o.df.head()\n",
    "\n",
    "                     number  occupied\n",
    "2019-10-09 17:00:00       1         1\n",
    "2019-10-09 17:00:10       1         1\n",
    "2019-10-09 17:00:20       1         1\n",
    "2019-10-09 17:00:30       1         1\n",
    "2019-10-09 17:00:40       1         1\n",
    "2019-10-09 17:00:50       1         1\n",
    "2019-10-09 17:01:00       1         1\n",
    "2019-10-09 17:01:10       1         1\n",
    "2019-10-09 17:01:20       1         1\n",
    "2019-10-09 17:01:30       1         1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomeOccupancy(HomeData):\n",
    "    \n",
    "    def __init__(self, path, freq = '10S'):      \n",
    "        HomeData.__init__(self, path) \n",
    "        self.ground_path = os.path.join(self.root_dir, 'GroundTruth')\n",
    "        self.occ_freq = freq    \n",
    "        self.occupant_names = []\n",
    "\n",
    "    def mylistdir(self, directory):\n",
    "        filelist = os.listdir(directory)\n",
    "        return [x for x in filelist if x.endswith('.csv')]        \n",
    "        \n",
    "    def get_ground_truth(self):\n",
    "        occupant_files = self.mylistdir(self.ground_path)\n",
    "        occupants = {}\n",
    "        enter_times, exit_times = [], []\n",
    "        \n",
    "        for occ in occupant_files:\n",
    "            occupant_name = occ.strip('.csv').split('-')[1] ## all homes, eg H1-maggie\n",
    "#             occupant_name = occ.strip('.csv').split('-')[0]  ## historic - maggie-H1\n",
    "            self.occupant_names.append(occupant_name)\n",
    "            ishome = []\n",
    "            with open(os.path.join(self.ground_path, occ)) as csv_file:\n",
    "                csv_reader, line_count = csv.reader(csv_file, delimiter=','), 0\n",
    "                for row in csv_reader:\n",
    "                    status, when = row[1], row[2].split('at')\n",
    "                    dt_day = datetime.strptime(str(when[0] + when[1]), '%B %d, %Y  %I:%M%p')\n",
    "                    ishome.append((status, dt_day))\n",
    "                    if line_count == 0:\n",
    "                        enter_times.append(dt_day)\n",
    "                    line_count += 1\n",
    "                exit_times.append(dt_day)\n",
    "                \n",
    "            occupants[occupant_name] = ishome        \n",
    "        self.first_last = (sorted(enter_times)[0], sorted(exit_times)[-1])\n",
    "        print(self.occupant_names)\n",
    "        return occupants\n",
    "    \n",
    "    def create_occupancy_df(self, occupants, frequency):\n",
    "        occ_range = pd.date_range(start=self.first_last[0], end=self.first_last[1], freq=frequency)    \n",
    "        occ_df = pd.DataFrame(index=occ_range)\n",
    "        \n",
    "        for occ in occupants:\n",
    "            occ_df[occ] = 99\n",
    "            s1 = 'exited'\n",
    "            for r in occupants[occ]:\n",
    "                date = r[1]\n",
    "                s2 = r[0]                \n",
    "                occ_df.loc[(occ_df.index < date) & (occ_df[occ]==99) & (s1 == 'exited') & (s2 == 'entered'), occ] =  0\n",
    "                occ_df.loc[(occ_df.index < date) & (occ_df[occ]==99) & (s1 == 'entered') & (s2 == 'exited'), occ] =  1\n",
    "                s1 = s2               \n",
    "            occ_df.loc[(occ_df.index >= date) & (occ_df[occ] == 99) & (s1 == 'entered'), occ] = 1\n",
    "            occ_df.loc[(occ_df.index >= date) & (occ_df[occ] == 99) & (s1 == 'exited'), occ] = 0    \n",
    "            \n",
    "        occ_df['number'] = occ_df[list(occupants.keys())].sum(axis = 1)\n",
    "        occ_df['occupied'] = 0\n",
    "        occ_df.loc[occ_df['number'] > 0, 'occupied'] = 1\n",
    "        return (occ_df)\n",
    "    \n",
    "    \n",
    "    def average_df(self, df):\n",
    "        time_series = []\n",
    "        for group, df_chunk in df.groupby(np.arange(len(df))//self.average_length):\n",
    "            df_max = df_chunk.max()\n",
    "            df_index = df_chunk.iloc[-1]\n",
    "            time_series.append(df_index.name)\n",
    "            df_summary = df_max.to_frame().transpose() \n",
    "            new_df = df_summary if group == 0 else pd.concat([new_df, df_summary])\n",
    "\n",
    "        new_df.index = time_series  \n",
    "        new_df = new_df[['number', 'occupied']]\n",
    "        return new_df\n",
    "\n",
    "    \n",
    "#     def not_average_df(self, df):\n",
    "#         time_series = []\n",
    "#         for group, df_chunk in df.groupby(np.arange(len(df))//self.average_length):\n",
    "#             df_max = df_chunk.max()\n",
    "#             df_index = df_chunk.iloc[-1]\n",
    "#             time_series.append(df_index.name)\n",
    "#             df_summary = df_max.to_frame().transpose() \n",
    "#             new_df = df_summary if group == 0 else pd.concat([new_df, df_summary])\n",
    "\n",
    "#         new_df.index = time_series  \n",
    "#         new_df = new_df[['number', 'occupied']]\n",
    "#         return new_df\n",
    "     \n",
    "       \n",
    "    def write_occupancy_csv(self, df, fname):       \n",
    "        fname = os.path.join(self.write_dir, fname)\n",
    "        if not os.path.isfile(fname):\n",
    "            df.to_csv(fname, index = True)\n",
    "            print(fname + ': Write Successful!')\n",
    "        else:\n",
    "            print(fname + ': File already exists')    \n",
    "\n",
    "            \n",
    "    def main(self):\n",
    "        occupant_status = self.get_ground_truth()\n",
    "        df = self.create_occupancy_df(occupant_status, frequency=self.occ_freq)\n",
    "        self.df = df[['number', 'occupied']]\n",
    "    \n",
    "\n",
    "        #self.df = self.average_df(df)        \n",
    "        #create larger grain for viewing (1 minute frequency)\n",
    "        write_df = self.create_occupancy_df(occupant_status, frequency='10S')\n",
    "        self.write_occupancy_csv(write_df, '{}-{}-Occupancy_df.csv'.format(self.home, self.system))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReadEnv class\n",
    "This reads in all the env data from the json files and writes summaries of the data for each hub\n",
    "Summaries are text files for each hub, with and entry for each day  which gives the start/end time \n",
    "of the files, and the perecentage of minutes that have at least 1 entry\n",
    "\n",
    "File is stored under ```Summary``` folder with title structure ```H1-BS1-data-summary.txt```\n",
    "\n",
    "eg:\n",
    "```\n",
    "BS1 2019-02-19 (00:00, 23:59) 0.93\n",
    "BS1 2019-02-20 (00:00, 12:14) 0.50\n",
    "BS1 2019-02-22 (21:55, 22:24) 0.02\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadEnv(HomeData):\n",
    "    \n",
    "    def __init__(self, path, sensor_hub):\n",
    "        HomeData.__init__(self, path)\n",
    "        self.name = sensor_hub\n",
    "        self.env_dir = os.path.join(self.root_dir, self.name, 'env_params')\n",
    "        self.from_pi = os.path.join(self.root_dir, self.name, 'env_from_pi')\n",
    "        self.num_folders = 288\n",
    "        self.files_per = 5\n",
    "        self.minutes_per_day = 1440\n",
    "        self.all_data = {}\n",
    "        self.first_last = {}\n",
    "        self.total_minutes = {}\n",
    "        self.details = []\n",
    "\n",
    "        \n",
    "    def get_date_folders(self, path):\n",
    "        date_folders = self.mylistdir(path)\n",
    "        date_folders.sort()\n",
    "        self.day1, self.dayn = date_folders[0], date_folders[-1]\n",
    "        return date_folders\n",
    "\n",
    "    def read_in_data(self, path):\n",
    "        with open(path, 'r') as f:\n",
    "            try:\n",
    "                self.data_dicts = json.loads(f.read())\n",
    "                for time_point in self.data_dicts:\n",
    "                    for measure in time_point:\n",
    "                        self.measurements[measure].append(time_point[measure])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "            \n",
    "    \n",
    "    def get_all_data(self, path, day):\n",
    "        self.measurements = {\n",
    "            'time':[], 'tvoc_ppb':[], 'temp_c':[], 'rh_percent':[], \n",
    "            'light_lux':[],'co2eq_ppm':[], 'dist_mm':[], 'co2eq_base':[], 'tvoc_base':[]}\n",
    "        file_path = os.path.join(path, day)\n",
    "        minute_folders = self.mylistdir(file_path)\n",
    "        minute_folders.sort()        \n",
    "        num_missing = 5 * (self.num_folders - len(minute_folders))\n",
    "        min_1, min_L = minute_folders[0], minute_folders[-1]\n",
    "        min_n = str(int(min_L) + 4).zfill(4)\n",
    "        self.first_last[day] = min_1, min_n\n",
    "        \n",
    "        for minute in minute_folders:\n",
    "            sub_files_path = os.path.join(file_path, minute)\n",
    "            sub_files = self.mylistdir(sub_files_path)\n",
    "            sub_files.sort()\n",
    "            missing = self.files_per - len(sub_files)\n",
    "            num_missing += missing\n",
    "            for file in sub_files:\n",
    "                if file.endswith('.json'):\n",
    "                    self.read_in_data(os.path.join(sub_files_path, file))\n",
    "        \n",
    "        self.all_data[day] = self.measurements\n",
    "        total_day = 1440 - num_missing\n",
    "        self.total_minutes[day] = total_day\n",
    "        \n",
    "    \n",
    "    def get_day_summary(self, day):\n",
    "        self.get_all_data(self.env_dir, day)\n",
    "        try:\n",
    "            total = self.total_minutes[day]/self.minutes_per_day\n",
    "            perc = '{:.2f}'.format(total)\n",
    "        except Exception as e:\n",
    "            print('except: {}'.format(e))\n",
    "            perc = 0.00\n",
    "        F1, F2 = self.first_last[day][0], self.first_last[day][1]\n",
    "        s = (f'({F1[0:2]}:{F1[2:4]}, {F2[0:2]}:{F2[2:4]})')\n",
    "        details = '{} {} {} {}'.format(self.name, day, s, perc)\n",
    "        return details, total\n",
    "\n",
    "      \n",
    "    def get_date_splits(self, dates):\n",
    "        dt_dates = [datetime.strptime(date, '%Y-%m-%d') for date in dates]\n",
    "        date_lists = self.date_segments(dt_dates)\n",
    "        all_lists = [[date.strftime('%Y-%m-%d') for date in sublist] for sublist in date_lists]\n",
    "        return all_lists\n",
    "    \n",
    "    \n",
    "    def read_all_days(self):\n",
    "        dates_to_use = []\n",
    "        date_folders = self.get_date_folders(self.env_dir)\n",
    "        fname = os.path.join(self.write_dir, '{}-{}-data-summary.txt'.format(self.home, self.name))\n",
    "        with open(fname, 'w+') as writer:\n",
    "            for day in date_folders:\n",
    "                day_details, total = self.get_day_summary(day)\n",
    "                #print(day_details)\n",
    "                writer.write(day_details + '\\n')\n",
    "                self.details.append(day_details)\n",
    "                if total > 0.85:\n",
    "                    dates_to_use.append(day)               \n",
    "            self.lists_of_dates = self.get_date_splits(dates_to_use)    \n",
    "        writer.close()\n",
    "        print(fname + ': Write Successful!')\n",
    "    \n",
    "   \n",
    "    def main(self):\n",
    "        self.read_all_days()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CleanEnvData class\n",
    "This performs the heavy lifting of processing the env params that have previously been read in and saved as a df.\n",
    "This also writes csvs to four different folder locations:\n",
    "\n",
    "- Under the ```Complete_CSV``` folder it creates a csv for each day and each hub\n",
    "\n",
    "- Under the ```Stacked_CSV``` folder it creates a csv for multiple days stacked together \n",
    "\n",
    "- Under the ```Stacked_CSV_10sec``` folder it creates the same as above, but with occupancy attached\n",
    "\n",
    "- Under the ```Stacked_CSV_5min``` folder it creates the same as above (with occupancy attached), but averaged over 5 minute periods. This frequency can be changed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanEnvData(HomeData):\n",
    "    \n",
    "    def __init__(self, path, lists, data, hubs, occ):\n",
    "        HomeData.__init__(self, path)\n",
    "        self.all_data = data\n",
    "        self.dates_in_common = lists\n",
    "        self.sensor_hubs = hubs\n",
    "        self.occupancy_df = occ\n",
    "        self.all_dfs = {}\n",
    "        self.var_names1 = ['tvoc_ppb', 'temp_c', 'rh_percent', 'light_lux', 'co2eq_ppm', 'dist_mm', 'abs_humid']\n",
    "        self.var_names2 = ['home', 'sensor']\n",
    "        \n",
    "    def csv_name(self, name, day):       \n",
    "        return str(self.home + '_' + name + '_' + day + '.csv')     \n",
    "        \n",
    "    def create_full_dfs(self, df, D1):\n",
    "        df_fullday = self.make_date_range(day1 = D1) ##use this for full 24hours\n",
    "        df2 = df.reindex(df_fullday, fill_value = np.nan) \n",
    "        df2.fillna(np.nan)\n",
    "        return df2\n",
    "        \n",
    "        \n",
    "    def make_date_range(self, day1, dayn=None, t1 = '0000', tn = '2359'):\n",
    "        self.range_start = str(day1 + ' ' + t1[0:2] + ':' + t1[2:4] + ':00')\n",
    "        self.range_end = str(day1 + ' ' + tn[0:2] + ':' + tn[2:4] + ':50')\n",
    "        date_range = pd.date_range(start=self.range_start, end=self.range_end, freq='10s')\n",
    "        return date_range \n",
    "        \n",
    "        \n",
    "    def clean_dates(self, name, day, df): \n",
    "        df['time'] = df['time'].str.strip('Z').str.replace('T',' ')\n",
    "        df['datetime_index'] = pd.to_datetime(df['time'])         \n",
    "        df = df.set_index('datetime_index')\n",
    "        df.index = df.index.floor('10s')\n",
    "        df2 = self.create_full_dfs(df, day)        \n",
    "        str_date = df2.index.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        df2.insert(loc = 0, column = 'str_datetime', value = str_date)\n",
    "        datetime_col = df2['str_datetime'].str.split(' ', n = 1, expand = True)         \n",
    "        df2.insert(loc = 0, column = 'date', value = datetime_col[0])\n",
    "        df2.insert(loc = 0, column = 'time-hr-min-sec', value = datetime_col[1])\n",
    "        time_col = datetime_col[1].str.split(':', n = 2, expand = True)    \n",
    "        df2.insert(loc = 0, column = 'second', value = time_col[2])\n",
    "        df2.insert(loc = 0, column = 'minute', value = time_col[1])\n",
    "        df2.insert(loc = 0, column = 'hour', value = time_col[0])        \n",
    "        df2 = df2.drop(columns = ['str_datetime', 'time'])\n",
    "        df2 = df2.sort_values(by = ['date', 'hour', 'minute', 'second'])\n",
    "        df2['home'] = self.home\n",
    "        df2['sensor'] = name\n",
    "        #df2 = df2.drop(columns = ['hour', 'minute', 'second', 'time-hr-min-sec', 'date'])\n",
    "\n",
    "        return df2     \n",
    "    \n",
    "    \n",
    "    def absolute_humidity(self, df):\n",
    "        df['abs_humid'] = 13.247*df['rh_percent']*(2.718281828459045**((17.67*df['temp_c'])/(243.5+df['temp_c']))/(273.15+df['temp_c']))\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def check_rh(self, df, day, limit=3000):\n",
    "        big_rh = df.loc[df.rh_percent > limit]\n",
    "        if len(big_rh) > 0:\n",
    "            print(big_rh)\n",
    "        else:\n",
    "            print('No high value rh for day {}'.format(day))\n",
    "        df.loc[df.rh_percent > limit, 'rh_percent'] = np.nan\n",
    "        return df \n",
    "    \n",
    "    \n",
    "#     def make_single_dfs(self):\n",
    "#         for sensor_hub in self.sensor_hubs:\n",
    "#             for date_list in self.dates_in_common:\n",
    "#                 #day1, dayn = date_list[0], date_list[-1]\n",
    "#                 for day in date_list:\n",
    "#                     new_df = pd.DataFrame.from_dict(self.all_data[sensor_hub][day])\n",
    "#                     new_df = self.absolute_humidity(new_df)\n",
    "#                     new_df = self.check_rh(new_df, day)  \n",
    "#                     clean_df = self.clean_dates(sensor_hub, day, new_df)\n",
    "#                     self.write_data(sensor_hub, day, clean_df)\n",
    "    \n",
    "    \n",
    "    def write_data(self, hub, df_to_write, folder, title):\n",
    "        storage_path = self.make_storage_directory(os.path.join(self.root_dir, hub, folder))\n",
    "        target_fname = os.path.join(storage_path, self.csv_name(hub, title)) \n",
    "        if not os.path.isfile(target_fname):\n",
    "            df_to_write.to_csv(target_fname, index_label = 'timestamp', index = True)\n",
    "            print(target_fname + ': Write Successful!')\n",
    "        else:\n",
    "            print(target_fname + ': File already exists')    \n",
    "            \n",
    "    \n",
    "    def join_dfs(self, date_list, sensor_hub):\n",
    "        df_list = []\n",
    "        for day in date_list:\n",
    "            new_df = pd.DataFrame.from_dict(self.all_data[sensor_hub][day])\n",
    "            new_df = self.absolute_humidity(new_df)\n",
    "            new_df = self.check_rh(new_df, day)  \n",
    "            clean_df = self.clean_dates(sensor_hub, day, new_df)\n",
    "            self.write_data(hub=sensor_hub, df_to_write=clean_df, folder='Complete_CSV', title = day)\n",
    "            df_list.append(clean_df)\n",
    "            \n",
    "        full_df = pd.concat(df_list)\n",
    "        full_df = full_df.ffill(limit = 30)\n",
    "        full_df = full_df.bfill(limit = 30) \n",
    "        full_df = full_df.drop(columns = ['hour', 'minute', 'second', 'time-hr-min-sec', 'date'])\n",
    "        \n",
    "        return full_df\n",
    "    \n",
    "    \n",
    "    def average_dfs(self, df):\n",
    "        time_series = []\n",
    "        \n",
    "        for group, df_chunk in df.groupby(np.arange(len(df))//self.average_length):\n",
    "            df_means = df_chunk[self.var_names1].mean()\n",
    "            df_index = df_chunk.iloc[-1][self.var_names2]\n",
    "            time_series.append(df_index.name)\n",
    "            sr_summary = df_index.append(df_means, ignore_index = False)\n",
    "            df_summary = sr_summary.to_frame().transpose()   \n",
    "            new_df = df_summary if group == 0 else pd.concat([new_df, df_summary])\n",
    "        \n",
    "        new_df.index = time_series\n",
    "        \n",
    "        return new_df\n",
    "    \n",
    "     \n",
    "    def attach_occupancy(self, df):\n",
    "        joined_df = pd.concat([df, self.occupancy_df], axis=1, join='inner')\n",
    "        return joined_df\n",
    "\n",
    "        \n",
    "        \n",
    "    def main(self): \n",
    "        for sensor_hub in self.sensor_hubs:\n",
    "            sensor_full_dfs = []\n",
    "            for date_list in self.dates_in_common:\n",
    "                day1, dayn = date_list[0], date_list[-1]\n",
    "                \n",
    "                full_df = self.join_dfs(date_list, sensor_hub)\n",
    "                self.write_data(hub=sensor_hub, df_to_write=full_df, folder='Stacked_CSV', title='{}_to_{}'.format(day1, dayn)) \n",
    "                \n",
    "                full_with_occ = self.attach_occupancy(full_df)\n",
    "                full_with_occ = full_with_occ.drop(columns = ['co2eq_base', 'tvoc_base'])\n",
    "                self.write_data(hub=sensor_hub, df_to_write=full_with_occ, folder='Stacked_CSV_10sec', title='{}_to_{}-occ'.format(day1, dayn)) \n",
    "#                 print(full_with_occ.columns)\n",
    "\n",
    "                averaged_df = self.average_dfs(full_df)\n",
    "#                 cols = averaged_df.columns.tolist()\n",
    "#                 cols = cols[2:] + cols[:2]\n",
    "#                 averaged_df = averaged_df[cols]               \n",
    "                df_w_occ = self.attach_occupancy(averaged_df)\n",
    "                self.write_data(hub=sensor_hub, df_to_write=df_w_occ, folder='Stacked_CSV_5min', title='{}_to_{}-averaged-occ'.format(day1, dayn))    \n",
    "\n",
    "                \n",
    "                #sensor_full_dfs.append(df_w_occ)               \n",
    "            self.all_dfs[sensor_hub] = sensor_full_dfs\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RS1', 'RS2', 'RS3', 'RS4', 'RS5']\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red'\n",
    "sensors_red = ['RS1', 'RS2', 'RS3', 'RS4', 'RS5']\n",
    "# sensors_black = ['BS2', 'BS3', 'BS4', 'BS5']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_dates_to_use' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-339b40c66c08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m Lists = [(tuple(r1), tuple(r2), tuple(r3), tuple(r4), tuple(r5)) \n\u001b[0;32m---> 16\u001b[0;31m          \u001b[0;32mfor\u001b[0m \u001b[0mr1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_dates_to_use\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RS1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m          \u001b[0;32mfor\u001b[0m \u001b[0mr2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_dates_to_use\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RS2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m          \u001b[0;32mfor\u001b[0m \u001b[0mr3\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_dates_to_use\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RS3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_dates_to_use' is not defined"
     ]
    }
   ],
   "source": [
    "# o = HomeOccupancy(path)\n",
    "# o.main()\n",
    "\n",
    "# all_sensor_data = {}\n",
    "# all_dates_to_use = {}\n",
    "# all_details = {}\n",
    "\n",
    "# for sensor in sensors_black:\n",
    "#     s = ReadEnv(path, sensor)\n",
    "#     s.main()\n",
    "#     all_sensor_data[sensor] = s.all_data\n",
    "#     all_dates_to_use[sensor] = s.lists_of_dates\n",
    "# #     all_details[sensor] = s.details\n",
    "\n",
    "Lists = [(tuple(r1), tuple(r2), tuple(r3), tuple(r4), tuple(r5)) \n",
    "         for r1 in all_dates_to_use['RS1']\n",
    "         for r2 in all_dates_to_use['RS2']\n",
    "         for r3 in all_dates_to_use['RS3']\n",
    "         for r4 in all_dates_to_use['RS4']\n",
    "         for r5 in all_dates_to_use['RS5']]   \n",
    "\n",
    "# Lists = [(tuple(b2), tuple(b3), tuple(b4), tuple(b5)) \n",
    "#          for b2 in all_dates_to_use['BS2']\n",
    "#          for b3 in all_dates_to_use['BS3']\n",
    "#          for b4 in all_dates_to_use['BS4']\n",
    "#          for b5 in all_dates_to_use['BS5']]    \n",
    "  \n",
    "        \n",
    "same_dates = []\n",
    "for L in Lists:\n",
    "    same_lists = set(L[0]).intersection(*L)\n",
    "    if len(same_lists) > 0:\n",
    "        same_dates.append(sorted(same_lists))\n",
    "\n",
    "print('\\n\\n*** There are {} lists ***\\n'.format(len(same_dates)))\n",
    "for l in same_dates:\n",
    "    print(l) \n",
    "\n",
    "\n",
    "\n",
    "e = CleanEnvData(path, same_dates, all_sensor_data, sensors_black, o.df)\n",
    "e.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Richard', 'Michael', 'Jay', 'Maggie']\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/Summaries/H4-red-Occupancy_df.csv: File already exists\n"
     ]
    }
   ],
   "source": [
    "o = HomeOccupancy(path)\n",
    "o.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.df['number'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/Summaries/H4-RS1-data-summary.txt: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/Summaries/H4-RS2-data-summary.txt: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/Summaries/H4-RS3-data-summary.txt: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/Summaries/H4-RS4-data-summary.txt: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/Summaries/H4-RS5-data-summary.txt: Write Successful!\n"
     ]
    }
   ],
   "source": [
    "all_sensor_data = {}\n",
    "all_dates_to_use = {}\n",
    "all_details = {}\n",
    "\n",
    "for sensor in sensors_red:\n",
    "    s = ReadEnv(path, sensor)\n",
    "    s.main()\n",
    "    all_sensor_data[sensor] = s.all_data\n",
    "    all_dates_to_use[sensor] = s.lists_of_dates\n",
    "    all_details[sensor] = s.details\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sensor RS1 list 1 has 1 dates\n",
      "['2019-04-19']\n",
      "\n",
      "Sensor RS1 list 2 has 16 dates\n",
      "['2019-04-28', '2019-04-29', '2019-04-30', '2019-05-01', '2019-05-02', '2019-05-03', '2019-05-04', '2019-05-05', '2019-05-06', '2019-05-07', '2019-05-08', '2019-05-09', '2019-05-10', '2019-05-11', '2019-05-12', '2019-05-13']\n",
      "\n",
      "Sensor RS1 list 3 has 5 dates\n",
      "['2019-05-17', '2019-05-18', '2019-05-19', '2019-05-20', '2019-05-21']\n",
      "\n",
      "Sensor RS2 list 1 has 1 dates\n",
      "['2019-04-19']\n",
      "\n",
      "Sensor RS2 list 2 has 2 dates\n",
      "['2019-05-01', '2019-05-02']\n",
      "\n",
      "Sensor RS2 list 3 has 11 dates\n",
      "['2019-05-04', '2019-05-05', '2019-05-06', '2019-05-07', '2019-05-08', '2019-05-09', '2019-05-10', '2019-05-11', '2019-05-12', '2019-05-13', '2019-05-14']\n",
      "\n",
      "Sensor RS2 list 4 has 5 dates\n",
      "['2019-05-17', '2019-05-18', '2019-05-19', '2019-05-20', '2019-05-21']\n",
      "\n",
      "Sensor RS3 list 1 has 1 dates\n",
      "['2019-04-28']\n",
      "\n",
      "Sensor RS3 list 2 has 15 dates\n",
      "['2019-04-30', '2019-05-01', '2019-05-02', '2019-05-03', '2019-05-04', '2019-05-05', '2019-05-06', '2019-05-07', '2019-05-08', '2019-05-09', '2019-05-10', '2019-05-11', '2019-05-12', '2019-05-13', '2019-05-14']\n",
      "\n",
      "Sensor RS3 list 3 has 5 dates\n",
      "['2019-05-17', '2019-05-18', '2019-05-19', '2019-05-20', '2019-05-21']\n",
      "\n",
      "Sensor RS4 list 1 has 2 dates\n",
      "['2019-05-01', '2019-05-02']\n",
      "\n",
      "Sensor RS4 list 2 has 2 dates\n",
      "['2019-05-04', '2019-05-05']\n",
      "\n",
      "Sensor RS4 list 3 has 7 dates\n",
      "['2019-05-08', '2019-05-09', '2019-05-10', '2019-05-11', '2019-05-12', '2019-05-13', '2019-05-14']\n",
      "\n",
      "Sensor RS4 list 4 has 5 dates\n",
      "['2019-05-17', '2019-05-18', '2019-05-19', '2019-05-20', '2019-05-21']\n",
      "\n",
      "Sensor RS5 list 1 has 1 dates\n",
      "['2019-04-28']\n",
      "\n",
      "Sensor RS5 list 2 has 14 dates\n",
      "['2019-05-01', '2019-05-02', '2019-05-03', '2019-05-04', '2019-05-05', '2019-05-06', '2019-05-07', '2019-05-08', '2019-05-09', '2019-05-10', '2019-05-11', '2019-05-12', '2019-05-13', '2019-05-14']\n",
      "\n",
      "Sensor RS5 list 3 has 5 dates\n",
      "['2019-05-17', '2019-05-18', '2019-05-19', '2019-05-20', '2019-05-21']\n",
      "\n",
      "\n",
      "*** There are 4 lists ***\n",
      "\n",
      "['2019-05-01', '2019-05-02']\n",
      "['2019-05-04', '2019-05-05']\n",
      "['2019-05-08', '2019-05-09', '2019-05-10', '2019-05-11', '2019-05-12', '2019-05-13']\n",
      "['2019-05-17', '2019-05-18', '2019-05-19', '2019-05-20', '2019-05-21']\n"
     ]
    }
   ],
   "source": [
    "Lists = [(tuple(r1), tuple(r2), tuple(r3), tuple(r4), tuple(r5)) \n",
    "         for r1 in all_dates_to_use['RS1']\n",
    "         for r2 in all_dates_to_use['RS2']\n",
    "         for r3 in all_dates_to_use['RS3']\n",
    "         for r4 in all_dates_to_use['RS4']\n",
    "         for r5 in all_dates_to_use['RS5']]   \n",
    "  \n",
    "\n",
    "# Lists = [(tuple(b2), tuple(b3), tuple(b4), tuple(b5)) \n",
    "#          for b2 in all_dates_to_use['BS2']\n",
    "#          for b3 in all_dates_to_use['BS3']\n",
    "#          for b4 in all_dates_to_use['BS4']\n",
    "#          for b5 in all_dates_to_use['BS5']]  \n",
    "\n",
    "for s in all_dates_to_use:\n",
    "    for i, l in enumerate(all_dates_to_use[s]):\n",
    "        print('\\nSensor {} list {} has {} dates'.format(s, i+1, len(l)))\n",
    "        print(l)    \n",
    "        \n",
    "same_dates = []\n",
    "for L in Lists:\n",
    "    same_lists = set(L[0]).intersection(*L)\n",
    "    if len(same_lists) > 0:\n",
    "        same_dates.append(sorted(same_lists))\n",
    "\n",
    "print('\\n\\n*** There are {} lists ***\\n'.format(len(same_dates)))\n",
    "for l in same_dates:\n",
    "    print(l) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2019-05-01', '2019-05-02'], ['2019-05-04', '2019-05-05'], ['2019-05-08', '2019-05-09', '2019-05-10', '2019-05-11', '2019-05-12', '2019-05-13'], ['2019-05-17', '2019-05-18', '2019-05-19', '2019-05-20', '2019-05-21']]\n",
      "No high value rh for day 2019-05-01\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Complete_CSV/H4_RS1_2019-05-01.csv: Write Successful!\n",
      "No high value rh for day 2019-05-02\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Complete_CSV/H4_RS1_2019-05-02.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Stacked_CSV/H4_RS1_2019-05-01_to_2019-05-02.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Stacked_CSV_10sec/H4_RS1_2019-05-01_to_2019-05-02-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Stacked_CSV_5min/H4_RS1_2019-05-01_to_2019-05-02-averaged-occ.csv: Write Successful!\n",
      "                      time  tvoc_ppb  temp_c   rh_percent  light_lux  \\\n",
      "3054  2019-05-04T08:44:00Z        25    13.4  3288.300049     9884.0   \n",
      "\n",
      "      co2eq_ppm  dist_mm  co2eq_base  tvoc_base  abs_humid  \n",
      "3054       1354       53       35186      35588  382.09064  \n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Complete_CSV/H4_RS1_2019-05-04.csv: Write Successful!\n",
      "                      time  tvoc_ppb  temp_c  rh_percent  light_lux  \\\n",
      "7043  2019-05-05T20:03:50Z         0    14.9      3285.5      177.0   \n",
      "\n",
      "      co2eq_ppm  dist_mm  co2eq_base  tvoc_base   abs_humid  \n",
      "7043        400       81       35197      36017  418.554956  \n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Complete_CSV/H4_RS1_2019-05-05.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Stacked_CSV/H4_RS1_2019-05-04_to_2019-05-05.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Stacked_CSV_10sec/H4_RS1_2019-05-04_to_2019-05-05-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Stacked_CSV_5min/H4_RS1_2019-05-04_to_2019-05-05-averaged-occ.csv: Write Successful!\n",
      "No high value rh for day 2019-05-08\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Complete_CSV/H4_RS1_2019-05-08.csv: Write Successful!\n",
      "No high value rh for day 2019-05-09\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Complete_CSV/H4_RS1_2019-05-09.csv: Write Successful!\n",
      "No high value rh for day 2019-05-10\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Complete_CSV/H4_RS1_2019-05-10.csv: Write Successful!\n",
      "                      time  tvoc_ppb  temp_c  rh_percent  light_lux  \\\n",
      "4216  2019-05-11T12:09:41Z         0    13.6      3288.5     2693.0   \n",
      "\n",
      "      co2eq_ppm  dist_mm  co2eq_base  tvoc_base   abs_humid  \n",
      "4216        400       78       35198      36081  386.854883  \n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Complete_CSV/H4_RS1_2019-05-11.csv: Write Successful!\n",
      "No high value rh for day 2019-05-12\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Complete_CSV/H4_RS1_2019-05-12.csv: Write Successful!\n",
      "No high value rh for day 2019-05-13\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Complete_CSV/H4_RS1_2019-05-13.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Stacked_CSV/H4_RS1_2019-05-08_to_2019-05-13.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Stacked_CSV_10sec/H4_RS1_2019-05-08_to_2019-05-13-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Stacked_CSV_5min/H4_RS1_2019-05-08_to_2019-05-13-averaged-occ.csv: Write Successful!\n",
      "No high value rh for day 2019-05-17\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Complete_CSV/H4_RS1_2019-05-17.csv: Write Successful!\n",
      "No high value rh for day 2019-05-18\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Complete_CSV/H4_RS1_2019-05-18.csv: Write Successful!\n",
      "No high value rh for day 2019-05-19\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Complete_CSV/H4_RS1_2019-05-19.csv: Write Successful!\n",
      "No high value rh for day 2019-05-20\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Complete_CSV/H4_RS1_2019-05-20.csv: Write Successful!\n",
      "No high value rh for day 2019-05-21\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Complete_CSV/H4_RS1_2019-05-21.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Stacked_CSV/H4_RS1_2019-05-17_to_2019-05-21.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Stacked_CSV_10sec/H4_RS1_2019-05-17_to_2019-05-21-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS1/Stacked_CSV_5min/H4_RS1_2019-05-17_to_2019-05-21-averaged-occ.csv: Write Successful!\n",
      "                      time  tvoc_ppb  temp_c   rh_percent  light_lux  \\\n",
      "2339  2019-05-01T06:45:51Z         0    12.8  3288.699951      153.0   \n",
      "\n",
      "      co2eq_ppm  dist_mm  co2eq_base  tvoc_base   abs_humid  \n",
      "2339        400      809       35193      35835  368.215229  \n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Complete_CSV/H4_RS2_2019-05-01.csv: Write Successful!\n",
      "No high value rh for day 2019-05-02\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Complete_CSV/H4_RS2_2019-05-02.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Stacked_CSV/H4_RS2_2019-05-01_to_2019-05-02.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Stacked_CSV_10sec/H4_RS2_2019-05-01_to_2019-05-02-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Stacked_CSV_5min/H4_RS2_2019-05-01_to_2019-05-02-averaged-occ.csv: Write Successful!\n",
      "No high value rh for day 2019-05-04\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Complete_CSV/H4_RS2_2019-05-04.csv: Write Successful!\n",
      "No high value rh for day 2019-05-05\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Complete_CSV/H4_RS2_2019-05-05.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Stacked_CSV/H4_RS2_2019-05-04_to_2019-05-05.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Stacked_CSV_10sec/H4_RS2_2019-05-04_to_2019-05-05-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Stacked_CSV_5min/H4_RS2_2019-05-04_to_2019-05-05-averaged-occ.csv: Write Successful!\n",
      "No high value rh for day 2019-05-08\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Complete_CSV/H4_RS2_2019-05-08.csv: Write Successful!\n",
      "                      time  tvoc_ppb  temp_c   rh_percent  light_lux  \\\n",
      "4523  2019-05-09T13:06:51Z         0    12.8  3288.800049     8403.0   \n",
      "\n",
      "      co2eq_ppm  dist_mm  co2eq_base  tvoc_base   abs_humid  \n",
      "4523        400     1189       35192      35764  368.226436  \n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Complete_CSV/H4_RS2_2019-05-09.csv: Write Successful!\n",
      "                      time  tvoc_ppb  temp_c  rh_percent  light_lux  \\\n",
      "6331  2019-05-10T18:12:30Z         0    13.1      3289.0     3775.0   \n",
      "6475  2019-05-10T18:36:31Z         0    13.1      3289.0     3387.0   \n",
      "\n",
      "      co2eq_ppm  dist_mm  co2eq_base  tvoc_base   abs_humid  \n",
      "6331        400     1180       35195      35907  375.154214  \n",
      "6475        400      812       35196      35981  375.154214  \n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Complete_CSV/H4_RS2_2019-05-10.csv: Write Successful!\n",
      "No high value rh for day 2019-05-11\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Complete_CSV/H4_RS2_2019-05-11.csv: Write Successful!\n",
      "No high value rh for day 2019-05-12\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Complete_CSV/H4_RS2_2019-05-12.csv: Write Successful!\n",
      "                     time  tvoc_ppb  temp_c   rh_percent  light_lux  \\\n",
      "160  2019-05-13T00:31:41Z         0    14.2  3287.899902        0.0   \n",
      "\n",
      "     co2eq_ppm  dist_mm  co2eq_base  tvoc_base   abs_humid  \n",
      "160        400      820       35201      36207  401.312924  \n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Complete_CSV/H4_RS2_2019-05-13.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Stacked_CSV/H4_RS2_2019-05-08_to_2019-05-13.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Stacked_CSV_10sec/H4_RS2_2019-05-08_to_2019-05-13-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Stacked_CSV_5min/H4_RS2_2019-05-08_to_2019-05-13-averaged-occ.csv: Write Successful!\n",
      "No high value rh for day 2019-05-17\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Complete_CSV/H4_RS2_2019-05-17.csv: Write Successful!\n",
      "No high value rh for day 2019-05-18\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Complete_CSV/H4_RS2_2019-05-18.csv: Write Successful!\n",
      "                      time  tvoc_ppb  temp_c   rh_percent  light_lux  \\\n",
      "5015  2019-05-19T14:34:51Z         0    13.1  3288.500000     4818.0   \n",
      "5151  2019-05-19T14:57:31Z         0    13.1  3288.300049     2823.0   \n",
      "\n",
      "      co2eq_ppm  dist_mm  co2eq_base  tvoc_base   abs_humid  \n",
      "5015        400      844       35200      36171  375.097182  \n",
      "5151        400     1158       35201      36241  375.074375  \n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Complete_CSV/H4_RS2_2019-05-19.csv: Write Successful!\n",
      "No high value rh for day 2019-05-20\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Complete_CSV/H4_RS2_2019-05-20.csv: Write Successful!\n",
      "                      time  tvoc_ppb  temp_c   rh_percent  light_lux  \\\n",
      "3589  2019-05-21T10:32:11Z         0    13.2  3287.800049    14031.0   \n",
      "\n",
      "      co2eq_ppm  dist_mm  co2eq_base  tvoc_base   abs_humid  \n",
      "3589        400      734       35194      35854  377.343177  \n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Complete_CSV/H4_RS2_2019-05-21.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Stacked_CSV/H4_RS2_2019-05-17_to_2019-05-21.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Stacked_CSV_10sec/H4_RS2_2019-05-17_to_2019-05-21-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS2/Stacked_CSV_5min/H4_RS2_2019-05-17_to_2019-05-21-averaged-occ.csv: Write Successful!\n",
      "                      time  tvoc_ppb  temp_c   rh_percent  light_lux  \\\n",
      "7957  2019-05-01T23:01:11Z         0    12.6  3288.899902      153.0   \n",
      "\n",
      "      co2eq_ppm  dist_mm  co2eq_base  tvoc_base   abs_humid  \n",
      "7957        400     1851       35187      35503  363.695858  \n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Complete_CSV/H4_RS3_2019-05-01.csv: Write Successful!\n",
      "No high value rh for day 2019-05-02\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Complete_CSV/H4_RS3_2019-05-02.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Stacked_CSV/H4_RS3_2019-05-01_to_2019-05-02.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Stacked_CSV_10sec/H4_RS3_2019-05-01_to_2019-05-02-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Stacked_CSV_5min/H4_RS3_2019-05-01_to_2019-05-02-averaged-occ.csv: Write Successful!\n",
      "No high value rh for day 2019-05-04\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Complete_CSV/H4_RS3_2019-05-04.csv: Write Successful!\n",
      "No high value rh for day 2019-05-05\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Complete_CSV/H4_RS3_2019-05-05.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Stacked_CSV/H4_RS3_2019-05-04_to_2019-05-05.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Stacked_CSV_10sec/H4_RS3_2019-05-04_to_2019-05-05-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Stacked_CSV_5min/H4_RS3_2019-05-04_to_2019-05-05-averaged-occ.csv: Write Successful!\n",
      "No high value rh for day 2019-05-08\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Complete_CSV/H4_RS3_2019-05-08.csv: Write Successful!\n",
      "                      time  tvoc_ppb  temp_c   rh_percent  light_lux  \\\n",
      "1180  2019-05-09T03:23:40Z         0    12.5  3288.600098        0.0   \n",
      "\n",
      "      co2eq_ppm  dist_mm  co2eq_base  tvoc_base   abs_humid  \n",
      "1180        400     1925       35191      35711  361.410361  \n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Complete_CSV/H4_RS3_2019-05-09.csv: Write Successful!\n",
      "No high value rh for day 2019-05-10\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Complete_CSV/H4_RS3_2019-05-10.csv: Write Successful!\n",
      "No high value rh for day 2019-05-11\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Complete_CSV/H4_RS3_2019-05-11.csv: Write Successful!\n",
      "No high value rh for day 2019-05-12\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Complete_CSV/H4_RS3_2019-05-12.csv: Write Successful!\n",
      "No high value rh for day 2019-05-13\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Complete_CSV/H4_RS3_2019-05-13.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Stacked_CSV/H4_RS3_2019-05-08_to_2019-05-13.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Stacked_CSV_10sec/H4_RS3_2019-05-08_to_2019-05-13-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Stacked_CSV_5min/H4_RS3_2019-05-08_to_2019-05-13-averaged-occ.csv: Write Successful!\n",
      "No high value rh for day 2019-05-17\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Complete_CSV/H4_RS3_2019-05-17.csv: Write Successful!\n",
      "No high value rh for day 2019-05-18\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Complete_CSV/H4_RS3_2019-05-18.csv: Write Successful!\n",
      "No high value rh for day 2019-05-19\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Complete_CSV/H4_RS3_2019-05-19.csv: Write Successful!\n",
      "No high value rh for day 2019-05-20\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Complete_CSV/H4_RS3_2019-05-20.csv: Write Successful!\n",
      "                      time  tvoc_ppb  temp_c   rh_percent  light_lux  \\\n",
      "2336  2019-05-21T06:56:20Z         0    12.7  3287.399902     1042.0   \n",
      "7988  2019-05-21T23:16:20Z         0    12.6  3287.500000        0.0   \n",
      "\n",
      "      co2eq_ppm  dist_mm  co2eq_base  tvoc_base   abs_humid  \n",
      "2336        400        0       35191      35719  365.793688  \n",
      "7988        400     1837       35191      35716  363.541053  \n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Complete_CSV/H4_RS3_2019-05-21.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Stacked_CSV/H4_RS3_2019-05-17_to_2019-05-21.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Stacked_CSV_10sec/H4_RS3_2019-05-17_to_2019-05-21-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS3/Stacked_CSV_5min/H4_RS3_2019-05-17_to_2019-05-21-averaged-occ.csv: Write Successful!\n",
      "No high value rh for day 2019-05-01\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Complete_CSV/H4_RS4_2019-05-01.csv: Write Successful!\n",
      "No high value rh for day 2019-05-02\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Complete_CSV/H4_RS4_2019-05-02.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Stacked_CSV/H4_RS4_2019-05-01_to_2019-05-02.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Stacked_CSV_10sec/H4_RS4_2019-05-01_to_2019-05-02-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Stacked_CSV_5min/H4_RS4_2019-05-01_to_2019-05-02-averaged-occ.csv: Write Successful!\n",
      "No high value rh for day 2019-05-04\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Complete_CSV/H4_RS4_2019-05-04.csv: Write Successful!\n",
      "                      time  tvoc_ppb  temp_c  rh_percent  light_lux  \\\n",
      "7122  2019-05-05T20:20:00Z        38    15.2      3286.5     1289.0   \n",
      "\n",
      "      co2eq_ppm  dist_mm  co2eq_base  tvoc_base  abs_humid  \n",
      "7122        835     1924       35186      35501  426.40131  \n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Complete_CSV/H4_RS4_2019-05-05.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Stacked_CSV/H4_RS4_2019-05-04_to_2019-05-05.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Stacked_CSV_10sec/H4_RS4_2019-05-04_to_2019-05-05-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Stacked_CSV_5min/H4_RS4_2019-05-04_to_2019-05-05-averaged-occ.csv: Write Successful!\n",
      "No high value rh for day 2019-05-08\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Complete_CSV/H4_RS4_2019-05-08.csv: Write Successful!\n",
      "No high value rh for day 2019-05-09\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Complete_CSV/H4_RS4_2019-05-09.csv: Write Successful!\n",
      "No high value rh for day 2019-05-10\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Complete_CSV/H4_RS4_2019-05-10.csv: Write Successful!\n",
      "                      time  tvoc_ppb  temp_c   rh_percent  light_lux  \\\n",
      "1392  2019-05-11T04:05:00Z         0    13.0  3289.300049        0.0   \n",
      "\n",
      "      co2eq_ppm  dist_mm  co2eq_base  tvoc_base  abs_humid  \n",
      "1392        400     1950       35191      35733  372.87401  \n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Complete_CSV/H4_RS4_2019-05-11.csv: Write Successful!\n",
      "No high value rh for day 2019-05-12\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Complete_CSV/H4_RS4_2019-05-12.csv: Write Successful!\n",
      "No high value rh for day 2019-05-13\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Complete_CSV/H4_RS4_2019-05-13.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Stacked_CSV/H4_RS4_2019-05-08_to_2019-05-13.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Stacked_CSV_10sec/H4_RS4_2019-05-08_to_2019-05-13-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Stacked_CSV_5min/H4_RS4_2019-05-08_to_2019-05-13-averaged-occ.csv: Write Successful!\n",
      "No high value rh for day 2019-05-17\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Complete_CSV/H4_RS4_2019-05-17.csv: Write Successful!\n",
      "No high value rh for day 2019-05-18\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Complete_CSV/H4_RS4_2019-05-18.csv: Write Successful!\n",
      "                      time  tvoc_ppb  temp_c   rh_percent  light_lux  \\\n",
      "4687  2019-05-19T13:37:11Z         0    12.7  3289.399902     1085.0   \n",
      "\n",
      "      co2eq_ppm  dist_mm  co2eq_base  tvoc_base  abs_humid  \n",
      "4687        400     1958       35187      35506  366.01623  \n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Complete_CSV/H4_RS4_2019-05-19.csv: Write Successful!\n",
      "No high value rh for day 2019-05-20\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Complete_CSV/H4_RS4_2019-05-20.csv: Write Successful!\n",
      "                      time  tvoc_ppb  temp_c  rh_percent  light_lux  \\\n",
      "6566  2019-05-21T19:09:20Z         0    12.8      3289.5      168.0   \n",
      "\n",
      "      co2eq_ppm  dist_mm  co2eq_base  tvoc_base   abs_humid  \n",
      "6566        406     1959       35187      35522  368.304805  \n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Complete_CSV/H4_RS4_2019-05-21.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Stacked_CSV/H4_RS4_2019-05-17_to_2019-05-21.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Stacked_CSV_10sec/H4_RS4_2019-05-17_to_2019-05-21-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS4/Stacked_CSV_5min/H4_RS4_2019-05-17_to_2019-05-21-averaged-occ.csv: Write Successful!\n",
      "No high value rh for day 2019-05-01\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Complete_CSV/H4_RS5_2019-05-01.csv: Write Successful!\n",
      "No high value rh for day 2019-05-02\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Complete_CSV/H4_RS5_2019-05-02.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Stacked_CSV/H4_RS5_2019-05-01_to_2019-05-02.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Stacked_CSV_10sec/H4_RS5_2019-05-01_to_2019-05-02-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Stacked_CSV_5min/H4_RS5_2019-05-01_to_2019-05-02-averaged-occ.csv: Write Successful!\n",
      "No high value rh for day 2019-05-04\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Complete_CSV/H4_RS5_2019-05-04.csv: Write Successful!\n",
      "No high value rh for day 2019-05-05\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Complete_CSV/H4_RS5_2019-05-05.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Stacked_CSV/H4_RS5_2019-05-04_to_2019-05-05.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Stacked_CSV_10sec/H4_RS5_2019-05-04_to_2019-05-05-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Stacked_CSV_5min/H4_RS5_2019-05-04_to_2019-05-05-averaged-occ.csv: Write Successful!\n",
      "No high value rh for day 2019-05-08\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Complete_CSV/H4_RS5_2019-05-08.csv: Write Successful!\n",
      "No high value rh for day 2019-05-09\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Complete_CSV/H4_RS5_2019-05-09.csv: Write Successful!\n",
      "No high value rh for day 2019-05-10\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Complete_CSV/H4_RS5_2019-05-10.csv: Write Successful!\n",
      "No high value rh for day 2019-05-11\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Complete_CSV/H4_RS5_2019-05-11.csv: Write Successful!\n",
      "No high value rh for day 2019-05-12\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Complete_CSV/H4_RS5_2019-05-12.csv: Write Successful!\n",
      "No high value rh for day 2019-05-13\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Complete_CSV/H4_RS5_2019-05-13.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Stacked_CSV/H4_RS5_2019-05-08_to_2019-05-13.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Stacked_CSV_10sec/H4_RS5_2019-05-08_to_2019-05-13-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Stacked_CSV_5min/H4_RS5_2019-05-08_to_2019-05-13-averaged-occ.csv: Write Successful!\n",
      "No high value rh for day 2019-05-17\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Complete_CSV/H4_RS5_2019-05-17.csv: Write Successful!\n",
      "No high value rh for day 2019-05-18\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Complete_CSV/H4_RS5_2019-05-18.csv: Write Successful!\n",
      "No high value rh for day 2019-05-19\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Complete_CSV/H4_RS5_2019-05-19.csv: Write Successful!\n",
      "No high value rh for day 2019-05-20\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Complete_CSV/H4_RS5_2019-05-20.csv: Write Successful!\n",
      "No high value rh for day 2019-05-21\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Complete_CSV/H4_RS5_2019-05-21.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Stacked_CSV/H4_RS5_2019-05-17_to_2019-05-21.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Stacked_CSV_10sec/H4_RS5_2019-05-17_to_2019-05-21-occ.csv: Write Successful!\n",
      "/Users/maggie/Desktop/HPD_mobile_data/HPD-env/HPD_mobile-H4/H4-red/RS5/Stacked_CSV_5min/H4_RS5_2019-05-17_to_2019-05-21-averaged-occ.csv: Write Successful!\n"
     ]
    }
   ],
   "source": [
    "print(same_dates)\n",
    "\n",
    "e = CleanEnvData(path, same_dates, all_sensor_data, sensors_red, o.df)\n",
    "\n",
    "\n",
    "#e = CleanEnvData(path, same_dates, all_sensor_data, sensors_black, o.df)\n",
    "\n",
    "e.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RS1', 'RS2', 'RS3', 'RS4', 'RS5']\n",
      "['RS2', 'RS3', 'RS4', 'RS5']\n"
     ]
    }
   ],
   "source": [
    "sensors = sensors_red[1:]\n",
    "print(sensors_red)\n",
    "print(sensors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
